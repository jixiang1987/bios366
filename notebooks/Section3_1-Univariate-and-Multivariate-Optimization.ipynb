{
 "metadata": {
  "name": "",
  "signature": "sha256:bfe09f834820689d60527213feb1c039d6fcc242fcf9c99c3eac156cc8b63be2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Set some Pandas options\n",
      "pd.set_option('display.notebook_repr_html', False)\n",
      "pd.set_option('display.max_columns', 20)\n",
      "pd.set_option('display.max_rows', 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Optimization\n",
      "\n",
      "Optimization is the process of finding the *minima* or *maxima* of a function. Consider a function:\n",
      "\n",
      "$$f: \\mathbf{R} \\rightarrow \\mathbf{R}$$\n",
      "\n",
      "where $f',f''$ are continuous. A point $x^*$ is a *global* maximum if:\n",
      "\n",
      "$$f(x) \\le f(x^*) \\, \\forall \\, x$$\n",
      "\n",
      "or a *local* maximum if:\n",
      "\n",
      "$$f(x) \\le f(x^*) \\, \\forall \\, x:|x-x^*| \\lt \\epsilon$$\n",
      "\n",
      "Necessary conditions:\n",
      "\n",
      "1. $f'(x^*) = 0$\n",
      "2. $f''(x^*) \\le 0$ (sufficient if $f''(x^*) \\lt 0$)\n",
      "\n",
      "We will consider **local search** methods that generate a series of values that converge to the maximum:\n",
      "\n",
      "$$x_0, x_1, x_2, \\ldots \\rightarrow \\text{argmax}(f)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: Maximum Likelihood\n",
      "\n",
      "**Maximum likelihood** (ML) is an approach for estimating the parameters of statistical models. The resulting estimates from ML have good theoretical properties, so it is a widely-used method. \n",
      "\n",
      "There is a ton of theory regarding ML. We will restrict ourselves to the mechanics here.\n",
      "\n",
      "Say we have some data $y = y_1,y_2,\\ldots,y_n$ that is distributed according to some distribution:\n",
      "\n",
      "<div style=\"font-size: 120%;\">  \n",
      "$$Pr(Y_i=y_i | \\theta)$$\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, for example, is a **Poisson distribution** that describes the distribution of some discrete variables, typically *counts*: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.random.poisson(5, size=100)\n",
      "plt.hist(y, bins=12, normed=True)\n",
      "xlabel('y'); ylabel('Pr(y)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-eec3e70d4710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pr(y)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The product $\\prod_{i=1}^n Pr(y_i | \\theta)$ gives us a measure of how **likely** it is to observe values $y_1,\\ldots,y_n$ given the parameters $\\theta$. Maximum likelihood fitting consists of choosing the appropriate function $l= Pr(Y|\\theta)$ to maximize for a given set of observations. We call this function the *likelihood function*, because it is a measure of how likely the observations are if the model is true.\n",
      "\n",
      "> Given these data, how likely is this model?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above model, the data were drawn from a Poisson distribution with parameter $\\lambda =5$.\n",
      "\n",
      "$$L(y|\\lambda=5) = \\frac{e^{-5} 5^y}{y!}$$\n",
      "\n",
      "So, for any given value of $y$, we can calculate its likelihood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poisson_like = lambda x, lam: np.exp(-lam) * (lam**x) / (\n",
      "                                     np.arange(x)+1).prod()\n",
      "\n",
      "lam = 6\n",
      "value = 10\n",
      "poisson_like(value, lam)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or, for numerical stability, the log-likelihood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poisson_loglike = lambda x, lam: -lam + np.log(lam**x) - log(\n",
      "                                 (np.arange(x)+1).prod())\n",
      "\n",
      "poisson_loglike(value, lam)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(poisson_loglike(yi, lam) for yi in y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lam = 8\n",
      "np.sum(poisson_loglike(yi, lam) for yi in y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot the likelihood function for any value of the parameter(s):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lambdas = np.linspace(0,15)\n",
      "x = 5\n",
      "plt.plot(lambdas, [poisson_like(x, l) for l in lambdas])\n",
      "xlabel('$\\lambda$')\n",
      "ylabel('L($\\lambda$|x={0})'.format(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-5c3ee344b15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpoisson_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$\\lambda$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L($\\lambda$|x={0})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How is the likelihood function different than the probability distribution (or mass) function? The likelihood is a function of the parameter(s) *given the data*, whereas the PDF (PMF) returns the probability of data given a particular parameter value. Here is the PMF of the Poisson for $\\lambda=5$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lam = 5\n",
      "xvals = arange(15)\n",
      "plt.bar(xvals, [poisson_like(x, lam) for x in xvals])\n",
      "xlabel('x')\n",
      "ylabel('Pr(X|$\\lambda$=5)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'arange' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-417a4c4386ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpoisson_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pr(X|$\\lambda$=5)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'arange' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Why are we interested in the likelihood function? \n",
      "\n",
      "A reasonable estimate of the true, unknown value for the parameter is one which **maximizes the likelihood function**. So, inference is reduced to an optimization problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: Nashville rainfall data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider again the Nashville precipitation data (`nashville_precip.txt`), in your data directory, which records the monthly rainfall from 1871 to 2011. Since this is continuous, positive data, it may be possible to model it using a Gamma distribution. If so, we need to maximize:\n",
      "\n",
      "$$\\begin{align}l(\\alpha,\\beta) &= \\sum_{i=1}^n \\log[\\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}] \\cr \n",
      "&= n[(\\alpha-1)\\overline{\\log(x)} - \\bar{x}\\beta + \\alpha\\log(\\beta) - \\log\\Gamma(\\alpha)]\\end{align}$$\n",
      "\n",
      "where $n = 2012 \u2212 1871 = 141$ and the bar indicates an average over all *i*. We choose $\\alpha$ and $\\beta$ to maximize $l(\\alpha,\\beta)$.\n",
      "\n",
      "Notice $l$ is infinite if any $x$ is zero. We do not have any zeros, but we do have an NA value for one of the October data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding the MLE\n",
      "\n",
      "To find the maximum of any function, we typically take the *derivative* with respect to the variable to be maximized, set it to zero and solve for that variable. \n",
      "\n",
      "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\beta} = n\\left(\\frac{\\alpha}{\\beta} - \\bar{x}\\right) = 0$$\n",
      "\n",
      "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\alpha} = n\\left(\\log(\\beta) -\\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} + \\overline{\\log(x)}\\right) = 0$$\n",
      "\n",
      "So, in general we are finding:\n",
      "\n",
      "$$l(\\theta) = \\left(\\frac{d\\theta}{d\\theta_1}, \\cdots, \\frac{d\\theta}{d\\theta_n}\\right) = \\mathbf{0}$$\n",
      "\n",
      "Thus, we seek the root of the score equation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python, we can use Sympy to obtain the first derivative of the gamma likelihood with respect to $\\beta$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# via Sympy\n",
      "from sympy import *\n",
      "\n",
      "a, b, x = symbols('a b x')\n",
      "gamma_dist = (a-1)*log(x) - x*b + a*log(b) - log(gamma(a))\n",
      "diff(gamma_dist, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named sympy",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-b5c623700873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# via Sympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a b x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgamma_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named sympy"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which can be solved as $\\beta = \\alpha/\\bar{x}$. However, plugging this into the derivative with respect to $\\alpha$ yields:\n",
      "\n",
      "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\alpha} = \\log(\\alpha) + \\overline{\\log(x)} - \\log(\\bar{x}) - \\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} = 0$$\n",
      "\n",
      "This has no closed form solution. We must use ***numerical optimization***!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bisection method\n",
      "\n",
      "The bisection method performs numerical root-finding for univariate problems. It works on continuous functions on $[a_0, b_0]$ where:\n",
      "\n",
      "$$f\\prime(a_0)f\\prime(b_0) \\le 0$$\n",
      "\n",
      "which implies that there is a local maximum (minimum) between the two values somewhere.\n",
      "\n",
      "The initial interval $[a_0, b_0]$ is arbitrary, as long as the above condition is met.\n",
      "\n",
      "The algorithm iteratively shrinks the interval $[a_i,b_i]$, by subdividing it and retaining the subinterval for which the above condition is still true. If there are multiple optima, bisection will only find one of them.\n",
      "\n",
      "Due to a number of factors, including rounding error in floating point calculations, iterative procedures can easily run indefinitely. To avoid this, we specify a **stopping rule** that halts the algorithm based on some pre-defined convergence criteria.\n",
      "\n",
      "Since we are searching for a root, one approach is to monitor the convergence of $f\\prime((a_i + b_i)/2)$ to zero. However, when the likelihood is very flat, there can be very large changes in the estimated value even when $f\\prime(x)$ is small. Instead, we should monitor the difference in values from one iteration to the next.\n",
      "\n",
      "**absolute convergence criterion**:\n",
      "\n",
      "$$| x_{i+1} - x_{i} | \\lt \\epsilon$$\n",
      "\n",
      "where $\\epsilon$ is a chosen tolerance.\n",
      "\n",
      "**relative convergence criterion**:\n",
      "\n",
      "$$\\frac{| x_{i+1} - x_{i} |}{|x_{i}|} \\lt \\epsilon$$\n",
      "\n",
      "In general:\n",
      "\n",
      "* if values of $x$ are orders of magnitude larger (smaller) than $\\epsilon$, absolute convergence may stop too late (soon)\n",
      "* if the solution is too close to zero, relative convergence may become unstable\n",
      "\n",
      "\n",
      "Note that for the bisection method:\n",
      "\n",
      "$$b_t - a_t = \\frac{b_0 - a_0}{2^t}$$\n",
      "\n",
      "which means that for a given $\\epsilon$, we require: \n",
      "\n",
      "$$t > \\log_2\\left(\\frac{b_0 - a_0}{\\epsilon}\\right)$$\n",
      "\n",
      "Reducing $\\epsilon$ by a factor of 10 means increasing $t$ by $\\log_2(10) \\approx 3.3$.\n",
      "\n",
      "It is also wise to place an absolute limit on the number of iterations, in case the algorithm diverges or cycles."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bisection(func, a, b, tol=1e-9, max_iter=100):\n",
      "\n",
      "    # Check initial values\n",
      "    if a >= b:\n",
      "        raise ValueError('Right value must be larger than left')\n",
      "    \n",
      "    fa, fb = func([a, b])\n",
      "    \n",
      "    if fa*fb > 0:\n",
      "        raise ValueError('No maximum between specified values')\n",
      "    \n",
      "    i = 0\n",
      "    while (b - a) > tol:\n",
      "        \n",
      "        # More stable than c = (b + a)/2.\n",
      "        c = a + (b - a)/2.\n",
      "        fc = func(c)\n",
      "        \n",
      "        if fa*fc < 0:\n",
      "            b,fb = c,fc\n",
      "        else:\n",
      "            a,fa = c,fc\n",
      "            \n",
      "        i +=1\n",
      "        \n",
      "        if i == max_iter:\n",
      "            print('The algorithm did not converge in {0} iterations'.format(max_iter))\n",
      "            return(None)\n",
      "        \n",
      "    return((a+b)/2., i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To apply this algorithm to the precipitation data, we need a function for the derivative of the log likelihood with respect to one of the parameters, in this case, beta:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precip = pd.read_table(\"../data/nashville_precip.txt\", sep='\\s+')\n",
      "\n",
      "# Calculate statistics\n",
      "log_mean = precip.mean().apply(np.log)\n",
      "mean_log = precip.apply(np.log).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import psi\n",
      "\n",
      "def dlgamma(m, log_mean=log_mean[-1], mean_log=mean_log[-1]): \n",
      "    return np.log(m) - psi(m) - log_mean + mean_log"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where `log_mean` and `mean_log` are $\\log{\\bar{x}}$ and $\\overline{\\log(x)}$, respectively; `psi` is the logarithm of the gamma function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To obtain reasonable starting points, there are a handful of strategies:\n",
      "\n",
      "* plotting the function, and obtain a visual estimate\n",
      "* preliminary estimates via other approximate solutions\n",
      "* trial and error\n",
      "\n",
      "Here, we will plot the function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(1, 5, 100)\n",
      "y = dlgamma(x)\n",
      "plt.plot(x,y)\n",
      "plt.hlines(0, 1, 5, linestyles='--')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-34a36354ac4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bisection(dlgamma, 2, 4, tol=1e-7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Convergence Order\n",
      "\n",
      "We can quantify the efficiency of root-finding algorithms by calculating its **order of convergence**.\n",
      "\n",
      "If an algorithm satisfies $\\lim_{i \\rightarrow \\infty} \\epsilon_i = 0$, then the parameter $\\beta$ is the order of convergence according to:\n",
      "\n",
      "$$\\lim_{i \\rightarrow \\infty} \\frac{|\\epsilon_{i+1}|}{|\\epsilon_{i}|^{\\beta}} = c$$\n",
      "\n",
      "for some constant $c$. Larger values of $\\beta$ are the result of faster convergence.\n",
      "\n",
      "Note that there is usually a tradeoff of speed vs. robustness.\n",
      "\n",
      "The bisection method is unusual in that it does not meet the criterion for order of convergence, with the ratio above being possibly unbounded. In practice, it tends to exhibit approximately linear convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Fixed Point Iteration\n",
      "\n",
      "Another method for finding roots computationally is *fixed-point iteration*. A **fixed point** of a function is a point at which the evaluation of the point equals the point itself. Mathematically, for some function $g:\\mathbf{R} \\Rightarrow \\mathbf{R}$, a fixed point is:\n",
      "\n",
      "$$a \\in \\mathbf{R}: g(a) = a$$\n",
      "\n",
      "So, if we define our function $f$ as:\n",
      "\n",
      "$$f(x) = c(g(x)-x)$$\n",
      "\n",
      "then we can find the root of $f$ when we find the fixed point of $g$. Conversely:\n",
      "\n",
      "$$g(x) = cf(x) + x$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The method begins with an initial guess $x_0$, from which we generate $x_1 = g(x_0)$, then $x_2 = g(x_1)$, etc.\n",
      "\n",
      "In general, this is a 1st-order recurrence equation:\n",
      "\n",
      "$$x_{n+1} = g(x_{n})$$\n",
      "\n",
      "We can show that $x_n \\rightarrow a$ is a fixed point of $g$.\n",
      "\n",
      "$$a = \\lim_{n \\rightarrow \\infty} x_{n+1} = \\lim_{n \\rightarrow \\infty} g(x_n)$$\n",
      "$$ = g(\\lim_{n \\rightarrow \\infty} x_n) = a$$\n",
      "\n",
      "therefore, $a$ is a fixed point of $g$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unfortunately, this convergence does not always take place. For example, consider the following functions:\n",
      "\n",
      "$$g_1(x) = x^{1.5}, \\, g_2(x) = x^{0.75}$$\n",
      "\n",
      "both $g_1, g_2$ have fixed points at 1 (by inspection), but $g_1$ diverges.\n",
      "\n",
      "In general,\n",
      "\n",
      "* the algorithm converges if $|g'(a)<1|$, and diverges otherwise.\n",
      "* the initial value should be in the \"neighborhood\" of $x_0$ to guarantee convergence.\n",
      "\n",
      "In practice, we may *converge* to $a$ without reaching it. We want to stop when $x_n$ is \"close enough\":\n",
      "\n",
      "$$ |x_n - x_{n-1}| \\le \\epsilon$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Gamma MLE\n",
      "\n",
      "Implement a fixed point algorithm, and use it to calculate the gamma MLE for one of the months of precipitation in Nashville."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fp = lambda x:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Newton's Method\n",
      "\n",
      "Bracketing methods like bisection, which bounds a root within a sequence of intervals that gradually decrease in length, are slow relative to other approaches. A much faster root-finding method is **Newton's method**. \n",
      "\n",
      "If we have a function $f$ that is twice-differentiable, we can approximate the root of its derivative using a Taylor series expansion:\n",
      "\n",
      "$$f'(x^*) \\approx f'(x_i) + (x^* - x_i)f''(x_i) = 0$$\n",
      "\n",
      "We can approximate $f'$ by its tangent at $x_i$, and then approximate the root pf $f'$ by the root of the tangent line:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some function\n",
      "func = lambda x: 3./(1 + 400*np.exp(-2*x)) - 1\n",
      "xvals = np.linspace(0, 6)\n",
      "plt.plot(xvals, func(xvals))\n",
      "text(5.3, 2.1, '$f(x)$', fontsize=16)\n",
      "# zero line\n",
      "plt.plot([0,6], [0,0], 'k-')\n",
      "# value at step n\n",
      "plt.plot([4,4], [0,func(4)], 'k:')\n",
      "plt.text(4, -.2, '$x_n$', fontsize=16)\n",
      "# tangent line\n",
      "tanline = lambda x: -0.858 + 0.626*x\n",
      "plt.plot(xvals, tanline(xvals), 'r--')\n",
      "# point at step n+1\n",
      "xprime = 0.858/0.626\n",
      "plt.plot([xprime, xprime], [tanline(xprime), func(xprime)], 'k:')\n",
      "plt.text(xprime+.1, -.2, '$x_{n+1}$', fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-78b2e019a055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$f(x)$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# zero line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we solve for $x^*$, this gives:\n",
      "\n",
      "$$x^* = x_i - \\frac{f'(x_i)}{f''(x_i)}$$\n",
      "\n",
      "This result is just another approximation, however, so this formula is iterated as:\n",
      "\n",
      "$$x_{i+1} = x_i - \\frac{f'(x_i)}{f''(x_i)}$$\n",
      "\n",
      "In the specific case of a MLE problem, this becomes:\n",
      "\n",
      "$$\\theta_{i+1} = \\theta_i - \\frac{l'(\\theta_i)}{l''(\\theta_i)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Convergence\n",
      "\n",
      "Using Newton's algorithm, $x_i \\rightarrow x^*$ quickly, provided:\n",
      "\n",
      "1. $f'(x^*)=0$\n",
      "2. $f''(x^*)<0$\n",
      "3. $f''$ is *Lipschitz-continuous* in the neighborbood of $x^*$\n",
      "\n",
      "> Lipschitz continuous if there exists a $k$ such that:\n",
      ">\n",
      "> $|f''(x) - f''(y)| \\le k|x-y| \\, \\forall \\, x,y$\n",
      "\n",
      "The following implements Newton's method in Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def newt(f, f_prime, x0, tol=1e-9, max_iter=100, **kwargs):\n",
      "    \"\"\"\n",
      "    Newton-Raphson algorithm for solving f(x) == 0\n",
      "    \n",
      "    f: a function of a single variable, x\n",
      "    f_prime: a function that returns the derivative of f at x\n",
      "    x0: the initial guess at the fixed point\n",
      "    tol: minimum distance of consecutive guesses before algorithm stops\n",
      "    max_iter: maximum number of iterations to converge to tolerance before\n",
      "        algorithm stops\n",
      "    \"\"\"\n",
      "    \n",
      "    # Initialize\n",
      "    x = x0\n",
      "    fx, fpx = f(x, **kwargs), f_prime(x, **kwargs)\n",
      "    i = 0\n",
      "    \n",
      "    # Loop until conditions met\n",
      "    while (abs(fx) > tol) and (i < max_iter):\n",
      "        \n",
      "        x -= fx/fpx\n",
      "        fx, fpx = f(x), f_prime(x)\n",
      "        i += 1\n",
      "                \n",
      "    if abs(fx) > tol:\n",
      "        print \"Algorithm failed to converge\"\n",
      "        return None\n",
      "\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To apply the Newton-Raphson algorithm, we need a function that returns a vector containing the **first and second derivatives** of the function with respect to the variable of interest. We defined the first derivative above; the second derivative is as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import polygamma\n",
      "\n",
      "dl2gamma = lambda m, **kwargs: 1./m - polygamma(1, m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`polygamma` is a complex function of the gamma function that result when you take the second derivative of that function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Alpha MLE for December\n",
      "alpha_mle = newt(dlgamma, dl2gamma, 2, \n",
      "                 log_mean=log_mean[-1], mean_log=mean_log[-1])\n",
      "alpha_mle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "3.5189679152158253"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now plug this back into the solution for beta:\n",
      "\n",
      "<div style=\"font-size: 120%;\">  \n",
      "$$ \\beta  = \\frac{\\alpha}{\\bar{X}} $$\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta_mle = alpha_mle/precip.mean()[-1]\n",
      "beta_mle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.84261607547835782"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Newton's method is also available to us via SciPy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import newton"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For some common distributions, SciPy includes methods for fitting via MLE:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import gamma\n",
      "\n",
      "gamma.fit(precip.Dec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(2.2427517753152308, 0.65494604470188622, 1.570073932063466)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This fit is not directly comparable to our estimates, however, because SciPy's `gamma.fit` method fits an odd 3-parameter version of the gamma distribution. If we set the location parameter to zero:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scipy_params = gamma.fit(precip.Dec,  floc=0.)\n",
      "scipy_params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(3.5189679152399753, 0.0, 1.1867801114824814)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To complicate things further, SciPy's gamma distribution uses a parameterization that employs the inverse of beta for its scale parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1./scipy_params[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.84261607548414119"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multivariate Optimization\n",
      "\n",
      "We may wish to find the maximum of a function of several variables:\n",
      "\n",
      "$$f:\\mathbf{R}^k \\rightarrow \\mathbf{R}$$\n",
      "\n",
      "Assume:\n",
      "\n",
      "* first- and second-order partial derivatives exist, and are everywhere continuous.\n",
      "\n",
      "$$\\begin{align}\n",
      "\\mathbf{x} &= (x_1, x_2, \\ldots, x_k) \\cr\n",
      "&= x_1 \\mathbf{e}_1 + x_2 \\mathbf{e}_2, + \\ldots + x_k \\mathbf{e}_k\n",
      "\\end{align}$$\n",
      "\n",
      "where $\\mathbf{e}_i$ is the coordinate vector of element $i$.\n",
      "\n",
      "Denote the $i$th partial derivative with respect to $x_i$ as $f_i(x)=\\frac{\\partial f(\\mathbf{x})}{\\partial x_i}$.\n",
      "\n",
      "And we define the *gradient*:\n",
      "\n",
      "$$\\nabla f(\\mathbf{x}) = (f_1(\\mathbf{x}), \\ldots, f_k(\\mathbf{x}))'$$\n",
      "\n",
      "And the *hessian*:\n",
      "\n",
      "$$H(\\mathbf{x})  = \\left(\n",
      "\\begin{array}{c}\n",
      "  \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_1} & \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_1 \\partial x_k}  \\cr\n",
      "  \\vdots & \\ddots & \\vdots \\cr\n",
      "  \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_k \\partial x_1} & \\cdots & \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_k \\partial x_k}\n",
      "\\end{array}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Curvature\n",
      "\n",
      "For any vector other than $\\mathbf{0}$, the slope at $\\mathbf{x}$ in direction $\\mathbf{v}$ is given by:\n",
      "\n",
      "$$\\frac{\\mathbf{v}' \\nabla f(\\mathbf{x})}{||\\mathbf{v}||}$$\n",
      "\n",
      "where $||\\mathbf{v}||$ is the *euclidean norm* of $\\mathbf{v}$.\n",
      "\n",
      "$$||\\mathbf{v}|| = \\sqrt{v_1^2 + \\ldots + v_k^2} = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}$$\n",
      "\n",
      "The *curvature* at point $\\mathbf{x}$ is:\n",
      "\n",
      "$$\\frac{\\mathbf{v}' H(\\mathbf{x}) \\mathbf{v}}{||\\mathbf{v}||^2}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$f$ has a local maximum if, for all $i=1,\\ldots,k$ and sufficiently small $\\epsilon$:\n",
      "\n",
      "$$f(\\mathbf{x} + \\epsilon \\mathbf{e}_i) \\le f(\\mathbf{x})$$\n",
      "\n",
      "provided that:\n",
      "\n",
      "$$\\nabla f(\\mathbf{x}) = \\mathbf{0} = (0,\\ldots,0)'$$\n",
      "\n",
      "and the slope at $x$ in direction $v$ is $\\le 0$ (*necessary* condition). This implies that the Hessian is negative semi-definite.\n",
      "\n",
      "A *sufficient* condition is that the slope $v$ is $\\lt 0$ (negative definite Hessian).\n",
      "\n",
      "***How do we find the maximum?***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Steepest Ascent/Descent\n",
      "\n",
      "The steepest ascent (descent) method is a iterative search algorithm. For a function $f:\\mathbf{R}^k \\rightarrow \\mathbf{R}$ with continuous partial derivatives, we iterate via:\n",
      "\n",
      "$$\\mathbf{x}_{i+1} = \\mathbf{x}_i + \\alpha \\mathbf{v}_{i}^*$$\n",
      "\n",
      "where:\n",
      "\n",
      "* $\\alpha$ = positive scalar step size\n",
      "* $\\mathbf{v}_{i}^*$ = direction of largest slope at position $i$\n",
      "\n",
      "$$\\mathbf{v}_{i}^* = \\text{argmax}_{\\mathbf{v}} \\frac{\\mathbf{v}' \\nabla f(\\mathbf{x}_i)}{||\\mathbf{v}||}$$\n",
      "\n",
      "Questions:\n",
      "\n",
      "1. How do we find this direction?\n",
      "2. How big of a step do we take?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to maximize the slope, so consider the partial derivative with respect to $v_i$:\n",
      "\n",
      "$$\\frac{\\partial}{\\partial v_j} \\frac{\\mathbf{v}' \\nabla f(\\mathbf{x})}{||\\mathbf{v}||} = \\frac{f_j(\\mathbf{x})}{||\\mathbf{v}||} - \\frac{(\\mathbf{v}' \\nabla f(\\mathbf{x}))v_j}{||\\mathbf{v}||^3}$$\n",
      "\n",
      "Setting this equal to zero, we end up with:\n",
      "\n",
      "$$v_j \\propto f_j(\\mathbf{x})$$\n",
      "\n",
      "which implies that at point $\\mathbf{x}$, the direction with the largest slope is $\\nabla f(\\mathbf{x})$. So, steepest ascent is:\n",
      "\n",
      "$$\\mathbf{x}_{x+1} = \\mathbf{x}_i + \\alpha \\nabla f(\\mathbf{x}_i)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the appropriate step size $\\alpha$?\n",
      "\n",
      "* too large, and we risk over-shooting the maximum\n",
      "* too small, and the search is inefficient\n",
      "\n",
      "We want to choose $\\alpha$ to maximize:\n",
      "\n",
      "$$g(\\alpha) = f(\\mathbf{x}_i + \\alpha \\nabla f(\\mathbf{x}_i))$$\n",
      "\n",
      "We already know how to optimize univariate functions!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stopping conditions may include:\n",
      "\n",
      "* $||\\mathbf{x}_i - \\mathbf{x}_{i-1}|| \\le \\epsilon$\n",
      "* $|f(\\mathbf{x}_i) - f(\\mathbf{x}_{i-1})| \\le \\epsilon$\n",
      "* $||\\nabla f(\\mathbf{x}_i)|| \\le \\epsilon$\n",
      "\n",
      "for some small tolerance value $\\epsilon$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pseudocode\n",
      "\n",
      "Putting it all together, our steepest ascent algorithm should look something like this:\n",
      "\n",
      "1. Initialize $\\mathbf{x}_0$ and $\\mathbf{x}_1$\n",
      "2. Initialize counter $i=1$\n",
      "3. While $f(\\mathbf{x}_i - \\mathbf{x}_{i-1}) \\gt \\epsilon$:\n",
      "\n",
      "    * $\\mathbf{x}_{i-1} \\leftarrow \\mathbf{x}_i$\n",
      "    * Find optimal step size\n",
      "    * Calculate new $\\mathbf{x}_i = \\mathbf{x}_{i-1} + \\alpha \\nabla f(\\mathbf{x}_{i-1})$\n",
      "    * Increment $i$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function below implements gradient descent in Python."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize\n",
      "\n",
      "def gradient_descent(x0, f, f_prime, adapt=False):\n",
      "    x_i, y_i = x0\n",
      "    all_x_i = list()\n",
      "    all_y_i = list()\n",
      "    all_f_i = list()\n",
      "\n",
      "    for i in range(1, 100):\n",
      "        all_x_i.append(x_i)\n",
      "        all_y_i.append(y_i)\n",
      "        all_f_i.append(f([x_i, y_i]))\n",
      "        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n",
      "        if adapt:\n",
      "            # Compute a step size using a line_search\n",
      "            step = optimize.line_search(f, f_prime,\n",
      "                                np.r_[x_i, y_i], -np.r_[dx_i, dy_i],\n",
      "                                np.r_[dx_i, dy_i], c2=.05)\n",
      "            step = step[0]\n",
      "        else:\n",
      "            step = 1\n",
      "        x_i += -step*dx_i\n",
      "        y_i += -step*dy_i\n",
      "        if np.abs(all_f_i[-1]) < 1e-16:\n",
      "            break\n",
      "    return all_x_i, all_y_i, all_f_i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a simple example, we can use gradient descent to find the peak of a quadratic function, with varying conditioning."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def quad(epsilon, ndim=2):\n",
      "    def f(x):\n",
      "       x = np.asarray(x)\n",
      "       y = x.copy()\n",
      "       y *= np.power(epsilon, np.arange(ndim))\n",
      "       return .33*np.sum(y**2)\n",
      "\n",
      "    def f_prime(x):\n",
      "       x = np.asarray(x)\n",
      "       y = x.copy()\n",
      "       scaling = np.power(epsilon, np.arange(ndim))\n",
      "       y *= scaling\n",
      "       return .33*2*scaling*y\n",
      "\n",
      "    return f, f_prime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0, y0 = 1.6, 1.1\n",
      "\n",
      "f, f_prime = quad(0.8)\n",
      "gd_x_i, gd_y_i, gd_f_i = gradient_descent([x0, y0], f, f_prime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_opt(all_x_i, all_y_i, all_f_i, f):\n",
      "    \n",
      "    x_min, x_max = -1, 2\n",
      "    y_min, y_max = 2.25/3*x_min - .2, 2.25/3*x_max - .2\n",
      "    \n",
      "    levels = dict()\n",
      "    \n",
      "    # A formatter to print values on contours\n",
      "    def super_fmt(value):\n",
      "        if value > 1:\n",
      "            if np.abs(int(value) - value) < .1:\n",
      "                out = '$10^{%.1i}$' % value\n",
      "            else:\n",
      "                out = '$10^{%.1f}$' % value\n",
      "        else:\n",
      "            value = np.exp(value - .01)\n",
      "            if value > .1:\n",
      "                out = '%1.1f' % value\n",
      "            elif value > .01:\n",
      "                out = '%.2f' % value\n",
      "            else:\n",
      "                out = '%.2e' % value\n",
      "        return out\n",
      "    \n",
      "    # Plot the contour plot\n",
      "    if not max(all_y_i) < y_max:\n",
      "        x_min *= 1.2\n",
      "        x_max *= 1.2\n",
      "        y_min *= 1.2\n",
      "        y_max *= 1.2\n",
      "    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
      "    x = x.T\n",
      "    y = y.T\n",
      "    \n",
      "    #plt.figure()\n",
      "    #plt.clf()\n",
      "    #plt.axes([0, 0, 1, 1])\n",
      "    \n",
      "    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n",
      "    z = np.apply_along_axis(f, 0, X)\n",
      "    log_z = np.log(z + .01)\n",
      "    plt.imshow(log_z,\n",
      "            extent=[x_min, x_max, y_min, y_max],\n",
      "            cmap=plt.cm.gray_r, origin='lower',\n",
      "            vmax=log_z.min() + 1.5*log_z.ptp())\n",
      "    contours = plt.contour(log_z,\n",
      "                        levels=levels.get(f, None),\n",
      "                        extent=[x_min, x_max, y_min, y_max],\n",
      "                        cmap=plt.cm.gnuplot, origin='lower')\n",
      "    levels[f] = contours.levels\n",
      "    plt.clabel(contours, inline=1,\n",
      "                fmt=super_fmt, fontsize=14)\n",
      "    \n",
      "    plt.plot(all_x_i, all_y_i, 'b-', linewidth=2)\n",
      "    plt.plot(all_x_i, all_y_i, 'k+')\n",
      "    \n",
      "    plt.plot([0], [0], 'rx', markersize=12)\n",
      "    \n",
      "    \n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "    plt.xlim(x_min, x_max)\n",
      "    plt.ylim(y_min, y_max)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_opt(gd_x_i, gd_y_i, gd_f_i, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-d6847870b6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd_x_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd_y_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd_f_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-23-13cd3eb5a6fa>\u001b[0m in \u001b[0;36mplot_opt\u001b[0;34m(all_x_i, all_y_i, all_f_i, f)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlog_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     plt.imshow(log_z,\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Nelder-Mead Algorithm\n",
      "\n",
      "While there are a variety of gradient-based mutlivariate optimization methods at our disposal, for many problems of interest, first and second derivatives may not be available. The Nelder\u2013Mead algorithm is an alternative approach that uses direct search instead of gradient ascent. It works by refining a simplex, the generalization of intervals and triangles to high-dimensional spaces, to bracket the minimum.\n",
      "\n",
      "Since it is not based on gradients, it is robust when the target function is not smooth, though it is slower for smooth functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rosenbrock(x):\n",
      "    y = 4*x\n",
      "    y[0] += 1\n",
      "    y[1:] += 3\n",
      "    return np.sum(.5*(1 - y[:-1])**2 + (y[1:] - y[:-1]**2)**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize\n",
      "\n",
      "def nelder_mead(f, x0):\n",
      "    all_x_i = [x0[0]]\n",
      "    all_y_i = [x0[1]]\n",
      "    all_f_i = [f(x0)]\n",
      "    def store(X):\n",
      "        x, y = X\n",
      "        all_x_i.append(x)\n",
      "        all_y_i.append(y)\n",
      "        all_f_i.append(f(X))\n",
      "    optimize.fmin(f, x0, callback=store, ftol=1e-12)\n",
      "    return all_x_i, all_y_i, all_f_i\n",
      "\n",
      "nm_x_i, nm_y_i, nm_f_i = nelder_mead(rosenbrock, np.array([x0, y0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_opt(nm_x_i, nm_y_i, nm_f_i, rosenbrock)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: truncated distribution\n",
      "\n",
      "Suppose that we observe $Y$ truncated below at $a$ (where $a$ is known). If $X$ is the distribution of our observation, then:\n",
      "\n",
      "$$ P(X \\le x) = P(Y \\le x|Y \\gt a) = \\frac{P(a \\lt Y \\le x)}{P(Y \\gt a)}$$\n",
      "\n",
      "(so, $Y$ is the original variable and $X$ is the truncated variable) \n",
      "\n",
      "Then X has the density:\n",
      "\n",
      "$$f_X(x) = \\frac{f_Y (x)}{1\u2212F_Y (a)} \\, \\text{for} \\, x \\gt a$$ \n",
      "\n",
      "Suppose $Y \\sim N(\\mu, \\sigma^2)$ and $x_1,\\ldots,x_n$ are independent observations of $X$. We can use maximum likelihood to find $\\mu$ and $\\sigma$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we can simulate a truncated distribution using a `while` statement to eliminate samples that are outside the support of the truncated distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.random.normal(size=10000)\n",
      "a = -1\n",
      "x_small = x < a\n",
      "while x_small.sum():\n",
      "    x[x_small] = np.random.normal(size=x_small.sum())\n",
      "    x_small = x < a\n",
      "    \n",
      "_ = hist(x, bins=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can construct a log likelihood for this function using the conditional form:\n",
      "\n",
      "$$f_X(x) = \\frac{f_Y (x)}{1\u2212F_Y (a)} \\, \\text{for} \\, x \\gt a$$ "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.distributions import norm\n",
      "\n",
      "trunc_norm = lambda theta, a, x: -(np.log(norm.pdf(x, theta[0], theta[1])) - \n",
      "            np.log(1 - norm.cdf(a, theta[0], theta[1]))).sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this example, we will use another optimization algorithm, the **Nelder-Mead simplex algorithm**. It has a couple of advantages: \n",
      "\n",
      "- it does not require derivatives\n",
      "- it can optimize (minimize) a vector of parameters\n",
      "\n",
      "SciPy implements this algorithm in its `fmin` function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "optimize.fmin(trunc_norm, np.array([1,2]), args=(-1, x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In general, simulating data is a terrific way of testing your model before using it with real data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "Chapter 2 of [Givens, Geof H.; Hoeting, Jennifer A. (2012-10-09). Computational Statistics (Wiley Series in Computational Statistics)](http://www.stat.colostate.edu/computationalstatistics/)\n",
      "\n",
      "[Python Scientific Lecture Notes](http://scipy-lectures.github.io)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}